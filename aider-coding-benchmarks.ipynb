{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e3134e-7735-4482-b0f8-13b8fcbdf54d",
   "metadata": {},
   "source": [
    "# Aider coding benchmarks\n",
    "\n",
    "https://github.com/Aider-AI/aider/tree/main/benchmark\n",
    "\n",
    "https://github.com/Aider-AI/polyglot-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0b03b-846a-4bfd-a194-8badd870430c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T09:33:15.578130Z",
     "iopub.status.busy": "2025-05-08T09:33:15.577751Z",
     "iopub.status.idle": "2025-05-08T09:33:15.580484Z",
     "shell.execute_reply": "2025-05-08T09:33:15.579939Z",
     "shell.execute_reply.started": "2025-05-08T09:33:15.578116Z"
    }
   },
   "source": [
    "## 1. Setup for benchmarking\n",
    "\n",
    "Open a new terminal and execute the commands below:\n",
    "\n",
    "```bash\n",
    "cd wordslab-benchmarks/\n",
    "./build-aider-benchmarks-container.sh\n",
    "docker images\n",
    "```\n",
    "\n",
    "You should get the folowing result:\n",
    "\n",
    "```\n",
    "#/home/workspace/wordslab-benchmarks# docker images\n",
    "REPOSITORY        TAG       IMAGE ID       CREATED              SIZE\n",
    "aider-benchmark   latest    a3ead77fb6e4   About a minute ago   4.96GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10921f6-10e8-4f4e-a867-da4571a4a6d9",
   "metadata": {},
   "source": [
    "## 2. Run the benchmark\n",
    "\n",
    "Make sure that ollama is started with 8192 context length:\n",
    "\n",
    "```bash\n",
    "OLLAMA_HOST=0.0.0.0 OLLAMA_CONTEXT_LENGTH=8192 OLLAMA_LOAD_TIMEOUT=-1 ollama serve\n",
    "```\n",
    "\n",
    "Open a new terminal and execute the commands below:\n",
    "\n",
    "```bash\n",
    "cd wordslab-benchmarks\n",
    "./aider/benchmark/docker.sh\n",
    "```\n",
    "\n",
    "You should be logged in the Docker container:\n",
    "\n",
    "```\n",
    "/home/workspace/wordslab-benchmarks/aider/benchmark# ./docker.sh\n",
    "root@9adfdca74a91:/aider#\n",
    "```\n",
    "\n",
    "Inside the container, execute the following commands:\n",
    "\n",
    "```bash\n",
    "cd aider\n",
    "pip install -e .[dev]\n",
    "export AIDER_BENCHMARK_DIR=\"/aider/aider/tmp.benchmarks\"\n",
    "export OLLAMA_API_BASE=\"http://host.docker.internal:11434\"\n",
    "\n",
    "./benchmark/benchmark.py gemma3-4b-polyglot-run1 --model ollama_chat/gemma3:4b --edit-format whole --threads 1 --num-tests 10 --exercises-dir polyglot-benchmark\n",
    "./benchmark/benchmark.py gemma3-4b-python-run1 --model ollama_chat/gemma3:4b --edit-format whole --threads 1 --num-tests 10 --exercises-dir python-benchmark\n",
    "```\n",
    "\n",
    "IMPORTANT : add --cont to the command to resume a stalled run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe007ec-d18d-4ea1-9c7b-10faaf2c3dac",
   "metadata": {},
   "source": [
    "Fix the ollama mistral-small3.1 model to run on 24GB VRAM:\n",
    "\n",
    "```\n",
    "llama show --modelfile mistral-small3.1:24b > Modelfile\n",
    "vi Modelfile\n",
    "\n",
    "# Start with:      FROM mistral-small3.1:24b\n",
    "# ...\n",
    "# Add the line:    PARAMETER num_gpu 100\n",
    "\n",
    "ollama create mistral-small3.1:24b-gpu24GB -f Modelfile\n",
    "\n",
    "# eval rate:            18.08 tokens/s\n",
    "ollama run --verbose mistral-small3.1:24b\n",
    "\n",
    "# eval rate:            55.44 tokens/s\n",
    "ollama run --verbose mistral-small3.1:24b-gpu24GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7eca99-3697-40df-87b1-0df04c44af7d",
   "metadata": {},
   "source": [
    "## 3. Collect the results\n",
    "\n",
    "```\n",
    "- dirname: 2025-05-08-13-59-14--gemma3-4b-python-run6\n",
    "  test_cases: 129\n",
    "  model: ollama_chat/gemma3:4b\n",
    "  edit_format: whole\n",
    "  commit_hash: 8956eef-dirty\n",
    "  pass_rate_1: 21.7\n",
    "  pass_rate_2: 24.0\n",
    "  pass_num_1: 28\n",
    "  pass_num_2: 31\n",
    "  percent_cases_well_formed: 96.1\n",
    "  error_outputs: 19\n",
    "  num_malformed_responses: 19\n",
    "  num_with_malformed_responses: 5\n",
    "  user_asks: 49\n",
    "  lazy_comments: 0\n",
    "  syntax_errors: 0\n",
    "  indentation_errors: 0\n",
    "  exhausted_context_windows: 0\n",
    "  test_timeouts: 0\n",
    "  total_tests: 133\n",
    "  command: aider --model ollama_chat/gemma3:4b\n",
    "  date: 2025-05-08\n",
    "  versions: 0.82.4.dev\n",
    "  seconds_per_case: 20.3\n",
    "  total_cost: 0.0000\n",
    "\n",
    "- dirname: 2025-05-08-21-06-37--gemma3-12b-python-run1\n",
    "  test_cases: 133\n",
    "  model: ollama_chat/gemma3:12b\n",
    "  edit_format: whole\n",
    "  commit_hash: 8956eef-dirty\n",
    "  pass_rate_1: 34.6\n",
    "  pass_rate_2: 42.1\n",
    "  pass_num_1: 46\n",
    "  pass_num_2: 56\n",
    "  percent_cases_well_formed: 100.0\n",
    "  error_outputs: 0\n",
    "  num_malformed_responses: 0\n",
    "  num_with_malformed_responses: 0\n",
    "  user_asks: 4\n",
    "  lazy_comments: 0\n",
    "  syntax_errors: 0\n",
    "  indentation_errors: 0\n",
    "  exhausted_context_windows: 0\n",
    "  test_timeouts: 1\n",
    "  total_tests: 133\n",
    "  command: aider --model ollama_chat/gemma3:12b\n",
    "  date: 2025-05-08\n",
    "  versions: 0.82.4.dev\n",
    "  seconds_per_case: 23.9\n",
    "  total_cost: 0.0000\n",
    "\n",
    "- dirname: 2025-05-09-04-24-18--gemma3-27b-python-run1\n",
    "  test_cases: 133\n",
    "  model: ollama_chat/gemma3:27b\n",
    "  edit_format: whole\n",
    "  commit_hash: 8956eef-dirty\n",
    "  pass_rate_1: 39.1\n",
    "  pass_rate_2: 48.9\n",
    "  pass_num_1: 52\n",
    "  pass_num_2: 65\n",
    "  percent_cases_well_formed: 98.5\n",
    "  error_outputs: 6\n",
    "  num_malformed_responses: 3\n",
    "  num_with_malformed_responses: 2\n",
    "  user_asks: 14\n",
    "  lazy_comments: 0\n",
    "  syntax_errors: 0\n",
    "  indentation_errors: 0\n",
    "  exhausted_context_windows: 0\n",
    "  test_timeouts: 0\n",
    "  total_tests: 133\n",
    "  command: aider --model ollama_chat/gemma3:27b\n",
    "  date: 2025-05-09\n",
    "  versions: 0.82.4.dev\n",
    "  seconds_per_case: 217.3\n",
    "  total_cost: 0.0000\n",
    "\n",
    "- dirname: 2025-05-10-06-26-52--qwen2.5-coder-7b-python-run1\n",
    "  test_cases: 133\n",
    "  model: ollama_chat/qwen2.5-coder:7b\n",
    "  edit_format: whole\n",
    "  commit_hash: 8956eef-dirty\n",
    "  pass_rate_1: 44.4\n",
    "  pass_rate_2: 51.1\n",
    "  pass_num_1: 59\n",
    "  pass_num_2: 68\n",
    "  percent_cases_well_formed: 100.0\n",
    "  error_outputs: 1\n",
    "  num_malformed_responses: 0\n",
    "  num_with_malformed_responses: 0\n",
    "  user_asks: 9\n",
    "  lazy_comments: 0\n",
    "  syntax_errors: 0\n",
    "  indentation_errors: 0\n",
    "  exhausted_context_windows: 0\n",
    "  test_timeouts: 2\n",
    "  total_tests: 133\n",
    "  command: aider --model ollama_chat/qwen2.5-coder:7b\n",
    "  date: 2025-05-10\n",
    "  versions: 0.82.4.dev\n",
    "  seconds_per_case: 21.7\n",
    "  total_cost: 0.0000\n",
    "\n",
    "- dirname: 2025-05-09-21-41-07--qwen2.5-coder-14b-python-run1\n",
    "  test_cases: 133\n",
    "  model: ollama_chat/qwen2.5-coder:14b\n",
    "  edit_format: whole\n",
    "  commit_hash: 8956eef-dirty\n",
    "  pass_rate_1: 54.1\n",
    "  pass_rate_2: 66.9\n",
    "  pass_num_1: 72\n",
    "  pass_num_2: 89\n",
    "  percent_cases_well_formed: 100.0\n",
    "  error_outputs: 0\n",
    "  num_malformed_responses: 0\n",
    "  num_with_malformed_responses: 0\n",
    "  user_asks: 13\n",
    "  lazy_comments: 0\n",
    "  syntax_errors: 0\n",
    "  indentation_errors: 0\n",
    "  exhausted_context_windows: 0\n",
    "  test_timeouts: 1\n",
    "  total_tests: 133\n",
    "  command: aider --model ollama_chat/qwen2.5-coder:14b\n",
    "  date: 2025-05-09\n",
    "  versions: 0.82.4.dev\n",
    "  seconds_per_case: 27.0\n",
    "  total_cost: 0.0000\n",
    "\n",
    "- dirname: 2025-05-10-06-50-30--qwen2.5-coder-32b-python-run1\n",
    "  test_cases: 133\n",
    "  model: ollama_chat/qwen2.5-coder:32b\n",
    "  edit_format: whole\n",
    "  commit_hash: 8956eef-dirty\n",
    "  pass_rate_1: 56.4\n",
    "  pass_rate_2: 72.2\n",
    "  pass_num_1: 75\n",
    "  pass_num_2: 96\n",
    "  percent_cases_well_formed: 100.0\n",
    "  error_outputs: 0\n",
    "  num_malformed_responses: 0\n",
    "  num_with_malformed_responses: 0\n",
    "  user_asks: 5\n",
    "  lazy_comments: 0\n",
    "  syntax_errors: 0\n",
    "  indentation_errors: 0\n",
    "  exhausted_context_windows: 0\n",
    "  test_timeouts: 1\n",
    "  total_tests: 133\n",
    "  command: aider --model ollama_chat/qwen2.5-coder:32b\n",
    "  date: 2025-05-10\n",
    "  versions: 0.82.4.dev\n",
    "  seconds_per_case: 50.0\n",
    "  total_cost: 0.0000\n",
    "\n",
    "costs: $0.0000/test-case, $0.00 total, $0.00 projected\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c318c-c73f-442d-a279-6d29cf9780b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-benchmarks",
   "language": "python",
   "name": "wordslab-benchmarks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
